     1	91-05/VR.5dollars.paper
     2	
     3	As appeared in: Proceedings of the ACM SIGCHI Human Factors in Computer
     4	Systems Conference, April, 1991, New Orleans
     5	
     6	
     7	                   Virtual Reality on Five Dollars a Day
     8	
     9	                                Randy Pausch
    10	                        Computer Science Department
    11	                           University of Virginia
    12	                               Thornton Hall
    13	                         Charlottesville, VA 22903
    14	                            Pausch@Virginia.edu
    15	
    16	ABSTRACT
    17	
    18	Virtual reality systems using head-mounted displays and glove input are
    19	gaining popularity but their cost prohibits widespread use. We have
    20	developed a system using an 80386 IBM-PCTM, a Polhemus 3Space IsotrakTM, two
    21	Reflection Technology Private EyeTM displays, and a Mattel Power GloveTM.
    22	For less than $5,000, we have created an effective vehicle for developing
    23	interaction techniques in virtual reality. Our system displays monochrome
    24	wire frames of objects with a spatial resolution of 720 by 280, the highest
    25	resolution head-mounted system published to date. We have confirmed findings
    26	by other researchers that low-latency interaction is significantly more
    27	important than high-quality graphics or stereoscopy. We have also found it
    28	useful to display reference objects to our user, specifically a ground plane
    29	for reference and a vehicle containing the user.
    30	
    31	KEYWORDS: Virtual reality, head-mounted display, glove input, computer
    32	graphics, teleoperation, speech recognition, hand gesturing,
    33	three-dimensional interaction.
    34	
    35	INTRODUCTION
    36	
    37	Virtual reality systems are currently gaining popularity but the cost of the
    38	underlying hardware has limited research in the field. With any new
    39	technology, there is an early period where informal observations are made
    40	and large breakthroughs are possible. We believe that the best way to speed
    41	up this process with head-mounted display/glove input systems is to provide
    42	low cost versions of the technology so larger numbers of researchers may use
    43	it. We have developed a complete virtual reality system for less than
    44	$5,000, or less than five dollars per day if amortized over a three-year
    45	period. We built the system because we had an immediate need and also to
    46	show that virtual reality research can be done without expensive hardware.
    47	
    48	Our immediate interest in virtual reality interaction comes from the Tailor
    49	project[18], whose goal is to allow severely disabled children to control
    50	devices via gesture input. The Tailor system adjusts to each child's
    51	possible range of motion and converts motion in that range into analog
    52	control signals that drive software applications. To specify motion
    53	mappings, therapists with no technical background must specify one
    54	dimensional curves and two dimensional surfaces in three dimensional space.
    55	Using our low cost system, we will allow therapists to interactively
    56	manipulate a wire frame mesh by using the glove to grasp control points on
    57	the mesh.
    58	
    59	Our system provides 720 by 280 spatial resolution and weighs 6 ounces,
    60	making it higher resolution and lower weight than head-mounted displays
    61	previously reported in the literature. In this paper, we present several
    62	design observations made after experience with our system. Our first
    63	observation is that increasing spatial resolution does not greatly improve
    64	the quality of the system. We typically decrease our resolution to increase
    65	our rendering speed. We also observe that stereoscopy is not critical, and
    66	that reference objects such as a ground plane and a virtual vehicle are
    67	extremely helpful to the user.
    68	
    69	SYSTEM DESCRIPTION
    70	
    71	The main processor for our system is a 2.5 MIP, 20 Mhz 386-based IBM-PCTM
    72	compatible with 640K of RAM, a 80387 floating point co-processor, and
    73	MS-DOSTM. Our head-mounted display uses a combination of two Private Eye
    74	displays manufactured by Reflection Technology, Inc. [1]. Figure 1 shows a
    75	Private Eye, a 1.2 by 1.3 by 3.5 inch device weighing 2.5 ounces. The 1 inch
    76	square monochrome display surface has a resolution of 720 horizontal by 280
    77	vertical red pixels against a black background. Optics between the user's
    78	eye and the display surface make the image appear to be one to three feet
    79	wide, "floating" several feet away.
    80	
    81	The Private Eye is implemented with a vertical column of 280 red LEDs,
    82	manufactured as a unit to pack them as densely as possible. To fill the
    83	entire visual display area, the LEDs are switched on and off rapidly as a
    84	vibrating mirror rotates through the 720 different vertical columns of the
    85	display, as shown in Figure 2. The Private Eye can "shadow" a standard CGA
    86	display with resolution of either 640 by 200 or 320 by 200 pixels, or it can
    87	be accessed a library which supports a spatial resolution of 720 by 280
    88	resolution. The library allows the painting of text and bitmaps, but does
    89	not support graphics primitives such as lines; therefore, we use the device
    90	by shadowing a CGA display.
    91	
    92	Reflection Technologies is marketing the Private Eye primarily as a
    93	"hands-busy" display; Figure 3 shows how the company expects most users to
    94	wear the device. The user can look down into the display without obstructing
    95	normal vision. Figure 4 shows how we mount two Private Eyes underneath a
    96	baseball cap. We have also used sunglasses with leather sides to shield the
    97	user from peripheral distractions. Our head-mounted display can either be
    98	stereoscopic or bi-ocular (each eye receives the same picture).
    99	
   100	We use a Polhemus 3Space Isotrak[20] to track the position and orientation
   101	of the user's head. The Isotrak senses changes in a magnetic field and
   102	reports three spatial (x, y, z) and three angular (yaw, pitch, roll)
   103	coordinates 60 times each second. Our system uses the Mattel Power Glove as
   104	an input device for position and gesture information. The glove is
   105	manufactured by Mattel, Inc., under licence from Abrams-Gentile
   106	Entertainment, Inc. (AGE). The Power Glove is provided to retail stores at a
   107	wholesale cost of 62 dollars and is sold at a retail cost ranging between 70
   108	and 100 dollars. Although Mattel does not release unit sales figures, they
   109	report that in 1989 the Power Glove generated over 40 million dollars in
   110	revenue, implying that over half a million gloves were sold that year.
   111	
   112	Early glove research was conducted at VPL Research, Inc., the manufacturers
   113	of the DataGloveTM[23,27]. The DataGlove uses fiber optics to determine
   114	finger bend and a Polhemus tracker to determine hand position. Neither of
   115	these technologies could be mass produced easily, so the Power Glove uses
   116	variable resistance material for finger bend, and ultrasonics for hand
   117	position.
   118	
   119	The Power Glove is marketed as a peripheral for the Nintendo Entertainment
   120	SystemTM. To thwart rival toy manufacturers, the data stream between the
   121	Power Glove and the main Nintendo unit is encrypted. When the Power Glove
   122	was originally introduced, it was rumored that dozens of research groups
   123	across the country began working on decrypting this data stream, and that
   124	several groups actually broke the code. An article appeared in Byte magazine
   125	describing how to attach the glove as a serial device, but only allowed the
   126	glove to emulate a joystick-type input device[6]. Rather than engaging in
   127	cryptography, we phoned Chris Gentile at AGE and described our research
   128	goals. He allowed us to sign a non-disclosure agreement and within days sent
   129	us a decrypting device that allows us to use the glove as a serial device
   130	communicating over an RS232 line. AGE and VPL Research have recently
   131	announced the VPL/AGE Power Glove Education Support Program[26] and plan to
   132	provide a low cost glove with 5 degrees of freedom for between 150 and 200
   133	dollars.
   134	
   135	The Power Glove uses two ultrasonic transmitters on the back of the user's
   136	hand and three wall-mounted receivers configured in an L-shape. The glove
   137	communicates successfully within ten to fifteen feet of the receivers when
   138	it is oriented towards them. As the glove turns away from the receivers, the
   139	signals degrades. Although some signal is received up to a 90 degree angle,
   140	Mattel claims the glove is only usable at up to roughly 45 degrees. When the
   141	glove is within five to six feet of the receivers, its (x, y, z) coordinate
   142	information is accurate to within 0.25 inches [15]. In addition to position
   143	information, the Power Glove provides roll information, where roll is the
   144	angle made by pivoting the hand around the axis of the forearm. Roll is
   145	reported in one of twelve possible positions.
   146	
   147	Finger bend is determined from the varying resistance through materials
   148	running the length of the finger. The user's thumb, index, middle, and ring
   149	finger bend are each reported as a two-bit integer. This four-position
   150	granularity is significantly less than the resolution provided by the VPL
   151	DataGlove, but most of the gestures used in previously published virtual
   152	reality systems can be supported with only two bits per finger [2,8,11,25].
   153	
   154	The only hardware we plan to add to our system is for voice input. Several
   155	small vocabulary, speaker-dependent input devices exist for the PC, all
   156	costing several hundred dollars. Once this is added, many of the commands
   157	currently given by hand gesture will be replaced by voice input.
   158	
   159	All software for our system is locally developed in ANSI-standard C [12]. We
   160	have a simple version of PHIGS [10] and are using a locally developed user
   161	interface toolkit [17]. Our low-level graphics and input handling packages
   162	have been widely ported, and allow our students to develop applications on
   163	SunsTM, MacintoshesTM, or PCs before running them on the machine equipped
   164	with the head-mounted display. We are currently developing a
   165	three-dimensional glove-based object editor.
   166	
   167	Although fast enough to be used, the limiting factor of our system's
   168	performance is the speed of line scan conversion. We draw monochrome wire
   169	frame objects, but are limited by the hardware's ability to draw lines. The
   170	hardware can render 500 vectors per second (of random orientation and
   171	length) but our CPU can execute the floating point viewing transformations
   172	for 3,500 vectors per second. In practice, we tend to use scenes with
   173	roughly 50 lines and we sustain a rate of 7 frames per second.
   174	High-performance scan-conversion boards currently exist which would
   175	substantially improve our rendering capabilities, and we expect their price
   176	to drop substantially in the coming year.
   177	
   178	The major limitation of our system's usability is the lag of the Polhemus
   179	Isotrak. Other researchers using the Isotrak have also reported this
   180	problem; no one has precisely documented its duration, but it is within 150
   181	and 250 milliseconds[9]. Ascension Technology, Inc. recently announced the
   182	BirdTM, a $5,000 competitor to the Polhemus Isotrak with a lag of only 24
   183	milliseconds[21].
   184	
   185	The existing system, when augmented with voice, will still cost less than
   186	$5,000 in hardware ($750 for each eye, $3,000 for the head tracker, $80 for
   187	the Power Glove, and ~$400 for the voice input). For less than the cost of a
   188	high resolution color monitor, we have added the I/O devices to support a
   189	complete virtual reality system.
   190	
   191	RESEARCH OBSERVATIONS
   192	
   193	Fred Brooks [5] has commented that:
   194	
   195	"A major issue perplexes and bedevils the computer-human interface community
   196	-- the tension between narrow truths proved convincingly by statistically
   197	sound experiments, and broad `truths,' generally applicable, but supported
   198	only by possibly unrepresentative observations."
   199	
   200	Brooks distinguishes between findings, observations, and rules-of-thumb, and
   201	states that we should provide results in all three categories, as
   202	appropriate. Most research presented to date in virtual reality are either
   203	what Brooks calls observations or rules-of-thumb, and we continue in this
   204	vein, stating our experience:
   205	
   206	The quality of the graphics is not as important as the interaction latency
   207	
   208	If we had to choose between them, we would prefer to decrease our tracking
   209	lag than increase our graphics capabilities. Although we have much greater
   210	spatial resolution than other head-mounted displays, this does not seem to
   211	significantly improve the quality of our system. Our experience confirms
   212	what has been discovered at VPL Research and NASA AMES research center: if
   213	the display is driven by user head motion, users can tolerate low display
   214	resolution, but notice lag in the 200 millisecond range.
   215	
   216	Stereoscopy is not essential
   217	
   218	Users of bi-ocular and monocular (one eye covered with a patch) versions of
   219	our system could maneuver and interact with objects in the environment.
   220	Since a straightforward implementation of stereo viewing slows down graphics
   221	by a factor of two or doubles the hardware cost, it is not always an
   222	appropriate use of resources.
   223	
   224	A ground plane is extremely useful
   225	
   226	Non-head-mounted virtual worlds sometimes introduce a ground plane to
   227	provide orientation [3,22]. In expensive head-mounted systems, the floor is
   228	usually implicitly included as a shaded polygon. We found the need in our
   229	system to include an artificial ground plane for reference, drawn as a
   230	rectangular grid of either lines or dots.
   231	
   232	Display the limits of the "vehicle" to the user
   233	
   234	In virtual reality, a user's movement is always constrained by the physical
   235	world. In most systems this manifests with the user straining an umbilical
   236	cord. Even in systems with no umbilical and infinite range trackers, this
   237	problem will still exist. Unless the user is in the middle of a large, open
   238	space the real world will limit the user's motions. In the VIEW system [7,8]
   239	a waist-level hexagon displays the range of the tracker, but is part of the
   240	world scene and does not move as the user flies. We treat the user as always
   241	residing in a "vehicle" [24]. The vehicle for a Polhemus is roughly a ten
   242	foot hemisphere. If the user wishes to view an object within the range of
   243	the vehicle, he may walk over to it, thereby changing his own location
   244	within the vehicle. If, however, the user wishes to grab an object not
   245	currently in the vehicle, he must first fly the vehicle until the desired
   246	object is within the vehicle, as shown in Figure 5. Note that the user may
   247	be simultaneously moving within the vehicle and changing the vehicle's
   248	position in the virtual world, although in practice our users do not combine
   249	these operations. For small vehicles it is probably appropriate to always
   250	display their bounds but for larger vehicles it may be better to show their
   251	bounds only when users are near the edges.
   252	
   253	FUTURE WORK
   254	
   255	Adding voice input will allow us to experiment with a model we have
   256	developed to support object selection via simultaneous voice and gesture
   257	input. We have already built a prototype of this selection model using a
   258	display screen in combination with voice and gesture input and will attempt
   259	to repeat those results using a head-mounted display[19].
   260	
   261	We also will be addressing the registration problem, or the correct matching
   262	of real and synthetic objects. Until force-feedback technology improves from
   263	its current state[14,16], glove-based systems will have to use real-world
   264	objects as tactile and force feedback to the user for some tasks. For
   265	example, one could perform a virtual version of the popular magic trick
   266	"cups and balls" by moving real cups on a real table, but having arbitrary
   267	virtual objects appear under the cups. The graphics for the cups, which can
   268	be grasped and moved, must closely correspond to the real world cups. By
   269	attaching trackers to real world objects, we will study how closely the
   270	visual image must match reality to avoid user dissatisfaction. A second
   271	approach to this problem is to use the Private Eye as a heads up display,
   272	wearing it over only one eye and allowing the user to correlate the real
   273	world and synthetic graphics.
   274	
   275	We are currently pursuing support to create a laboratory with between ten
   276	and twenty low cost virtual reality stations. By providing reasonable access
   277	to an entire graduate or undergraduate class, we suspect we may quickly
   278	develop a large number of new interaction techniques. Jaron Lanier has
   279	commented that in virtual reality, "creativity is the only thing of value"
   280	[13]. A good way to spark creative breakthroughs is to increase the number
   281	of people actively using the technology. We are also exploring the
   282	possibility of creating a self-contained, portable system based on a laptop
   283	machine.
   284	
   285	CONCLUSIONS
   286	
   287	The field of virtual reality research is in its infancy, and will benefit
   288	greatly from putting the technology into as many researchers' hands as
   289	possible. The virtual reality systems previously described in the literature
   290	cost more than most researchers can afford. We have shown that for less than
   291	$5,000, or five dollars per day over three years, researchers can use a
   292	head-mounted display with glove and voice input. Our system has a higher
   293	spatial resolution than any previous system, and is significantly lighter
   294	than previous systems [4,7]. For glove input, the Power Glove has provided
   295	excellent spatial accuracy and usable finger bend data. Based on experience
   296	with our system, we have found that interaction latency is significantly
   297	more important than display resolution or stereoscopy, and that the user can
   298	greatly benefit from the display of reference objects, such as a ground
   299	plane and a virtual vehicle.
   300	
   301	ACKNOWLEDGMENTS
   302	
   303	This work could not have proceeded without the help we received from Chris
   304	Gentile of AGE. Novak of Mattel, Inc. also provided assistance with an early
   305	draft of the paper. We would also like to thank Ronald Williams, Pramod
   306	Dwivedi, Larry Ferber, Rich Gossweiler, and Chris Long at the University of
   307	Virginia for their help.
   308	
   309	REFERENCES
   310	
   311	1. Becker, A.,Design Case Study: Private Eye, Information Display, March,
   312	1990.
   313	
   314	2. Blanchard, C., Burgess, S., Harvill, Y., Lanier, J, and Lasko, A.,
   315	Reality Built for Two: A Virtual Reality Tool," ACM SIGGRAPH 1990 Symposium
   316	on Interactive 3D Graphics, March, 1990.
   317	
   318	3. Brett, C.,Pieper, S., and Zeltzer, D., Putting It All Together: An
   319	Integrated Package for Viewing and Editing 3D Microworlds, Proceedings of
   320	the 4th Usenix Computer Graphics Workshop, October, 1987.
   321	
   322	4. Brooks, F., Walkthrough - A Dynamic Graphics System for Simulating
   323	Virtual Buildings, Proceedings of the 1986 ACM Workshop on Interactive
   324	Graphics, October, 1986, 9-21.
   325	
   326	5. Brooks, F., Grasping Reality Through Illusion: Interactive Graphics
   327	Serving Science, Proceedings of the ACM SIGCHI Human Factors in Computer
   328	Systems Conference, Washington, D.C., May 17, 1988, 1-11.
   329	
   330	6. Eglowstein, H.,Reach Out and Touch Your Data, Byte, July 1990, 283-290.
   331	
   332	7. Fisher, S.,McGreevy, M.,Humphries, J., and Robinett, M., Virtual
   333	Environment Display System, Proceedings of the 1986 ACM Workshop on
   334	Interactive Graphics, October, 1986, 77-87.
   335	
   336	8. Fisher, S., The AMES Virtual Environment Workstation (VIEW), SIGGRAPH `89
   337	Course #29 Notes, August, 1989. (included a videotape).
   338	
   339	9. Fisher, S., Personal Communication (electronic mail), Crystal River,
   340	Inc., September 28, 1990.
   341	
   342	10. Foley, J., van Dam, A., Feiner, S., and Hughes, J., Computer Graphics,
   343	Principles and Practices, Addison-Wesley Publishing Co., 1990.
   344	
   345	11.  Kaufman, A., Yagel, R. and Bakalash, R., Direct Interaction with a 3D
   346	Volumetric Environment, ACM SIGGRAPH 1990 Symposium on Interactive 3D
   347	Graphics, March, 1990.
   348	
   349	12. Kelley, A. and Pohl, I., A Book on C, second Edition, Benjamin/Cummings
   350	Publishing Company, Inc., 1990.
   351	
   352	13. Lanier, J., Plenary Address on Virtual Reality, Proceedings of UIST: the
   353	Annual ACM SIGGRAPH Symposium on User Interface Software and Technology,
   354	November, 1989.
   355	
   356	14. Ming, O., Pique, M., Hughes, J., and Brooks, F., Force Display Performs
   357	Better than Visual Display in a Simple 6-D Docking Task, IEEE Robotics and
   358	Automation Conference, May, 1989.
   359	
   360	15. Novak, Personal Communication (telephone call), January 3, 1991.
   361	
   362	16. Ouh-young, M., Pique, M., Hughes, J., Srinivasan, N., and Brooks, F.,
   363	Using a Manipulator For Force Display in Molecular Docking, IEEE Robotics
   364	and Automation Conference 3 (April, 1988), 1824-1829.
   365	
   366	17. Pausch, R., A Tutorial for SUIT, the Simple User Interface Toolkit,
   367	Technical Report Tech. Rep.-90-29, University of Virginia Computer Science
   368	Department, September 1, 1990.
   369	
   370	18. Pausch, R., and Williams, R., Tailor: Creating Custom User Interfaces
   371	Based on Gesture, Proceedings of UIST: the Annual ACM SIGGRAPH Symposium on
   372	User Interface Software and Technology, October, 1990, 123-134.
   373	
   374	19. Pausch, R., and Gossweiler, R., "UserVerse: Application-Independent
   375	Object Selection Using Inaccurate Multi-Modal Input," in Multimedia and
   376	Multimodal User Interface Design, edited M. Blattner and R. Dannenberg,
   377	Addison-Wesley, 1991 (to appear).
   378	
   379	20. Rabb, F., Blood, E., Steiner, R., and. Jones, H., Magnetic Position and
   380	Orientation Tracking System, IEEE Transaction on Aerospace and Electronic
   381	Systems, 15, 5 (September, 1979), 709-718.
   382	
   383	21. Scully, J., Personal Communication (letter), Ascension Technology, Inc.,
   384	PO Box 527, Burlington, VT 05402 (802) 655-7879, June 27, 1990.
   385	
   386	22. Sturman, D., Pieper, S., and Zeltzer, D., Hands-on Interaction With
   387	Virtual Environments, Proceedings of UIST: the Annual ACM SIGGRAPH Symposium
   388	on User Interface Software and Technology, November, 1989.
   389	
   390	23. VPL-Research, DataGlove Model 2 Users Manual, Inc., 1987.
   391	
   392	24. Ware, C., and Osborne, S., Exploration and Virtual Camera Control in
   393	Virtual Three Dimensional Environments, ACM SIGGRAPH 1990 Symposium on
   394	Interactive 3D Graphics, March, 1990.
   395	
   396	25. Weimer, D., and Ganapathy, S., A Synthetic Visual Environment with Hand
   397	Gesturing and Voice Input, Proceedings of the ACM SIGCHI Human Factors in
   398	Computer Systems Conference, April, 1989, 235-240.
   399	
   400	26. Zachary, G., and Gentile, C., Personal Communication (letter), VPL
   401	Research, Inc., July 18, 1990. VPL/AGE Power Glove Support Program, VPL
   402	Research, Inc., 656 Bair Island Road, Suite 304, Redwood City, CA 94063,
   403	(415) 361-1710.
   404	
   405	27. Zimmerman, T., Lanier, J., Blanchard, C., Bryson, S., and Harvill, Y., A
   406	Hand Gesture Interface Device, Graphics Interface `87, May, 1987, 189-192.
   407	
   408	
   409	--
   410	--------------------------------------------------------------------
   411	Randy Pausch (Pausch@Virginia.Edu) 804-982-2211  FAX: (804) 982-2214
   412	Assistant Professor, Computer Science Department, Thornton Hall,
   413	University of Virginia, Charlottesville, VA 22903-2442 
   414	--------------------------------------------------------------------
   415	 